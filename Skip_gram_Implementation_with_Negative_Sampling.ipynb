{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip gram Implementation with Negative Sampling",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvXwitArnyli",
        "colab_type": "text"
      },
      "source": [
        "### What are Word Embeddings?\n",
        "\n",
        "Word embeddings are vectorized, fixed-length, distributedÂ , dense representations of words that interpret a word's textual meaning by mapping it to a vector of real values. Their incredible ability to represent words efficiently in vector space has made them *de facto* for NLP models. Word embeddings have gained enormous popularity and recognition since it's advent and are one of the most widely used building blocks in Natural Language Processing.\n",
        "\n",
        "### Is this Notebook any different?\n",
        "\n",
        "You'll find a lot of articles and notebooks focussing on word embeddings on the internet. But most of them dont offer much depth on how word embeddings are computed under the hood. They'd play with existing implmentations made avaiable by gensim, fasttext, GloVe, etc. While many say, word embeddings are easily available online so you'd hardly ever need to generate them on your own when you import the learned embeddings at a click of a button. Though it's a valid counter point, they're missing on the foundational knowledge and insights we gain from actually learning to encode the embeddings, visualizing the results of your learned embeddings. I'd highly recommend anyone starting with NLP to once work on building their very own word embeddings model.\n",
        "\n",
        "This notebook precisely provides an in-depth implementation for Word2Vec algorithm of generating word embeddings. We make use of Skip-gram architecture coupled with negative sampling for faster and efficient learning. Negative sampling will be explained in a little bit as you progress with the article.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoV5vSSSbIp0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "9ad7be99-eccd-4be9-fbe0-7cdb62cbd604"
      },
      "source": [
        "import io\n",
        "import math\n",
        "import gzip\n",
        "import nltk\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gensim.downloader as api\n",
        "import tensorflow_datasets as tfds\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from google.colab import files\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import skipgrams\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pwuegqx-JWf",
        "colab_type": "text"
      },
      "source": [
        "### Downloading Dataset\n",
        "We're going to use text8 dataset for the purpose of this article. Text8 is first 100,000,000 bytes of plain text from Wikipedia. It's mainly used for testing purposes. For preliminary results, we'd experiment with subset of the data and use the entire dataset once you're confident the model.\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG-FjuVEFLGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54a2b71f-2f1c-4099-99fc-2f939a0e7089"
      },
      "source": [
        "def load_data():\n",
        "  text8_zip_file_path = api.load('text8', return_path=True)\n",
        "  with gzip.open(text8_zip_file_path, 'rb') as file:\n",
        "    file_content = file.read()\n",
        "  wiki = file_content.decode()\n",
        "  return wiki\n",
        "\n",
        "wiki = load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYBT_-Vy_af4",
        "colab_type": "text"
      },
      "source": [
        "### Preprocessing data\n",
        "\n",
        "**Stopwords removal** - We begin with removing stopwords as they bring little to no value for our task of learning word embeddings. \n",
        "\n",
        "---\n",
        "\n",
        "**Subsampling words** - In a large corpora, most frequent words can easily occur hundreds of millions of times and such words usually don't bring much information to the table.  It is of essential importance to cut down on their frequencies to mitigate the negative impact it adds. For example, co-occurrences of \"English\" and \"Spanish\" benefit much more than co-occurrences of \"English\" and \"the\" or \"Spanish\" and \"of\". To counter the imbalance between rare and frequent words Mikolov et. al came up with the following heuristic formula for determining probability to drop a particular word:\n",
        "\n",
        "![formula.png](data:image/png;base64,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)\n",
        "\n",
        "where t is threshold value (heuristically set to 1e-5) and f(w) is frequency of the word.\n",
        "\n",
        "---\n",
        "\n",
        "**Filtering words** - Frequency of words tell us a lot about their importance and usability for our model. Words occuring only once can't really be representated properly because of the lack of context words associated with it. To preclude such noise from our data (as we don't have much information about their whereabouts), we're keeping words occuring atleast five time in our data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wp50T2OqA-7L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "dbcfc034-e0b5-433e-9d4a-1dc1bd7c3a66"
      },
      "source": [
        "def get_drop_prob(x, threshold_value):\n",
        "  return 1 - np.sqrt(threshold_value/x)\n",
        "\n",
        "def subsample_words(words, word_counts):\n",
        "  threshold_value = 1e-5\n",
        "  total_count = len(words)\n",
        "  freq_words = {word: (word_counts[word]/total_count) for word in set(words)}\n",
        "  subsampled_words = [word for word in words if random.random() < (1 - get_drop_prob(freq_words[word], threshold_value))]\n",
        "  return subsampled_words\n",
        "\n",
        "def preprocess_text(text):\n",
        "  # Replace punctuation with tokens so we can use them in our model\n",
        "  text = text.lower()\n",
        "  text = text.strip()\n",
        "  text = text.replace('.', ' <PERIOD> ')\n",
        "  text = text.replace(',', ' <COMMA> ')\n",
        "  text = text.replace('\"', ' <QUOTATION_MARK> ')\n",
        "  text = text.replace(';', ' <SEMICOLON> ')\n",
        "  text = text.replace('!', ' <EXCLAMATION_MARK> ')\n",
        "  text = text.replace('?', ' <QUESTION_MARK> ')\n",
        "  text = text.replace('(', ' <LEFT_PAREN> ')\n",
        "  text = text.replace(')', ' <RIGHT_PAREN> ')\n",
        "  text = text.replace('--', ' <HYPHENS> ')\n",
        "  text = text.replace('?', ' <QUESTION_MARK> ')\n",
        "  text = text.replace(':', ' <COLON> ')\n",
        "  words = text.split()\n",
        "\n",
        "  # Remove stopwords\n",
        "  stopwords_eng = set(stopwords.words('english'))\n",
        "  words = [word for word in words if word not in stopwords_eng]\n",
        "  # Remove all the words with frequency less than 5\n",
        "  word_counts = Counter(words)\n",
        "  print(\"Count of words: %s\" % (len(words)))\n",
        "  filtered_words = [word for word in words if word_counts[word] >= 5]\n",
        "  print(\"Count of filtered words: %s\" % (len(filtered_words)))\n",
        "  # Subsample words with threshold of 10^-5\n",
        "  subsampled_words = subsample_words(filtered_words, word_counts)\n",
        "  print(\"Count of subsampled words: %s\" % (len(subsampled_words)))\n",
        "\n",
        "  return word_counts, subsampled_words\n",
        "\n",
        "word_counts, preprocessed_words = preprocess_text(wiki[:5000000])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count of words: 535524\n",
            "Count of filtered words: 480253\n",
            "Count of subsampled words: 127200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4260B2ZFJ_EA",
        "colab_type": "text"
      },
      "source": [
        "It's always a good idea to take a quick look at preprocessed sample before heading further - you might observe few things that if handled can enrich or correct your data. More like a validation step this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oNvdt-v1dw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "preprocessed_words[1500:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCkFtaa_KrTb",
        "colab_type": "text"
      },
      "source": [
        "### Hyperparameters\n",
        "Setting a few hyperparamters required for gnerating batches and for deciding the size of word embeddings we wish to generate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJLzBkSIKoMx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EMBEDDING_DIM = 128\n",
        "BUFFER_SIZE = 1024\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oO7N0ZsLofI",
        "colab_type": "text"
      },
      "source": [
        "### Preparing Tensorflow Dataset using skipgrams\n",
        "\n",
        "**Generating skipgrams**\n",
        "\n",
        "First, we tokenize our pre-processed textual data and then convert them into corresponding vectorised tokens. After that, we make use *skipgrams* library offered by keras for generating (word, context) pairs. As it's description reads:\n",
        "\n",
        "Generates skipgram word pairs. It transforms a sequence of word indexes (list of integers) into tuples of words of the form:\n",
        "\n",
        "- (word, word in the same window), with label 1 (positive samples).  \n",
        "- (word, random word from the vocabulary), with label 0 (negative samples).  \n",
        "Read more about Skipgram in this gnomic paper by Mikolov et al.: [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781v3.pdf)\n",
        "\n",
        "**Negative Sampling**\n",
        "\n",
        "For every input we give to the network, we train it using the output from the softmax layer. That means for each input, we're making very small changes to millions of weights even though we only have one true example. This makes training the network very inefficient and unfeasible.\n",
        "The problem of predicting context words can instead be framed as a set of independent binary classification tasks. Then the goal is to independently predict the presence (or absence) of context words. The following snippet generates pairs of (target, context) words also known as skipgrams, and for each input(target, context) pair we also randomly sample a negative (target, ~context) pair."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq4-jfYOjonO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_counts)\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(preprocessed_words)\n",
        "vectorized_words = [tokenizer.word_index[word] for word in preprocessed_words]\n",
        "\n",
        "pairs, labels = skipgrams(vectorized_words, VOCAB_SIZE, window_size=3, negative_samples=1.0, shuffle=True)\n",
        "target_words = [p[0] for p in pairs]\n",
        "context_words = [q[1] for q in pairs]\n",
        "\n",
        "SAMPLE_SIZE = len(labels)\n",
        "labels_sample = labels[:SAMPLE_SIZE]\n",
        "target_words_sample = target_words[:SAMPLE_SIZE]\n",
        "context_words_sample = context_words[:SAMPLE_SIZE]\n",
        "train_size = int(len(labels_sample) * 0.9)\n",
        "train_target_words, train_context_words, train_labels = target_words_sample[:train_size], context_words_sample[:train_size], labels_sample[:train_size]\n",
        "test_target_words, test_context_words, test_labels = target_words_sample[train_size:], context_words_sample[train_size:], labels_sample[train_size:]\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_target_words, train_context_words, train_labels)).shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_target_words, test_context_words, test_labels)).shuffle(BUFFER_SIZE)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRHxw7X4zOpg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "93baea2c-a55c-432b-fbe3-831c5ba43785"
      },
      "source": [
        "print(\"# (train, test) batches: \" + str(len(list(train_dataset.as_numpy_iterator()))) + \", \" + str(len(list(test_dataset.as_numpy_iterator()))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# (train, test) batches: 21464, 2384\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUatOx50OXF1",
        "colab_type": "text"
      },
      "source": [
        "### Building Model\n",
        "Now let's build the model by using model subclassing method.  In the majority of situations, Sequential and Functional APIs are more appropriate, but you can still use model subclassing if you prefer to think in an object-oriented manner, as the typical Python/NumPy developer does."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6gLxFZ9Eu9Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SkipGramModel(tf.keras.Model):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "      super(SkipGramModel, self).__init__()\n",
        "      self.shared_embedding = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=1, name='word_embeddings')\n",
        "      self.flatten = tf.keras.layers.Flatten(name='flatten')\n",
        "      self.dense1 = tf.keras.layers.Dense(64, activation=tf.nn.relu, name='dense_one')\n",
        "      self.dropout1 = tf.keras.layers.Dropout(0.2, name = 'dropout1')\n",
        "      self.dense2 = tf.keras.layers.Dense(32, activation=tf.nn.relu, name='dense_two')\n",
        "      self.dropout2 = tf.keras.layers.Dropout(0.2, name = 'dropout2')\n",
        "      self.pred = tf.keras.layers.Dense(1, activation=tf.nn.sigmoid, name='predictions')\n",
        "\n",
        "    def call(self, target_word, context_word, training=True):\n",
        "      x = self.word_embedding(target_word)\n",
        "      y = self.word_embedding(context_word)\n",
        "      x = self.flatten(x)\n",
        "      y = self.flatten(y)\n",
        "      shared = tf.multiply(x, y)\n",
        "      dense_output1 = self.dense1(shared)\n",
        "      if training: dense_output1 = self.dropout1(dense_output1)\n",
        "      dense_output2 = self.dense2(dense_output1)\n",
        "      if training: dense_output2 = self.dropout2(dense_output2)\n",
        "      output = self.pred(dense_output2)\n",
        "      return tf.reshape(output, [-1])\n",
        "\n",
        "model = SkipGramModel(VOCAB_SIZE+1, EMBEDDING_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RN3SV3zv0pXG",
        "colab_type": "text"
      },
      "source": [
        "### Loss function, Metrics and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENLrMWOtpixA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimiser = tf.keras.optimizers.Adam()\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
        "val_acc_metric = tf.keras.metrics.BinaryAccuracy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eyQ_o1EWuJA",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model\n",
        "The model fit() method usually meets the requirements for training but custom training provides you finer control over optimization and other tasks associated to training. You could pick anyone depending on how complex your training's going to be. Here we have employed custom training for learning word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHNb85OL29hu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "outputId": "dcf63bef-59c9-4fe9-d15f-9bf4624815de"
      },
      "source": [
        "@tf.function\n",
        "def train_step(target_words, context_words, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "      preds = model(target_words, context_words)\n",
        "      loss = loss_fn(labels, preds)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimiser.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    train_acc_metric.update_state(labels, preds)\n",
        "    return loss\n",
        "\n",
        "@tf.function\n",
        "def test_step(target_words, context_words, labels):\n",
        "    preds = model(target_words, context_words, training=False)\n",
        "    loss = loss_fn(labels, preds)\n",
        "    val_acc_metric.update_state(labels, preds)\n",
        "    return loss\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start_time = time.time()\n",
        "  print(\"Starting epoch: %d \" % (epoch,))\n",
        "  cumm_loss = 0\n",
        "  for step, (target_words, context_words, labels) in enumerate(train_dataset):\n",
        "    train_loss = train_step(target_words, context_words, labels)\n",
        "    cumm_loss += train_loss\n",
        "  train_acc = train_acc_metric.result()\n",
        "  print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "  train_acc_metric.reset_states()\n",
        "  print(\"Cummulative loss: %.4f \" % (cumm_loss,))\n",
        "\n",
        "  test_cumm_loss = 0\n",
        "  for step, (target_words, context_words, labels) in enumerate(test_dataset):\n",
        "    test_loss = test_step(target_words, context_words, labels)\n",
        "    test_cumm_loss += test_loss\n",
        "  val_acc = val_acc_metric.result()\n",
        "  print(\"Validation acc over epoch: %.4f\" % (float(val_acc),))\n",
        "  val_acc_metric.reset_states()\n",
        "  print(\"Cummulative test loss: %f \" % (test_cumm_loss,))\n",
        "  print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch: 0 \n",
            "Training acc over epoch: 0.6382\n",
            "Cummulative loss: 13154.7461 \n",
            "Validation acc over epoch: 0.7458\n",
            "Cummulative test loss: 1226.445068 \n",
            "Time taken: 374.43s\n",
            "Starting epoch: 1 \n",
            "Training acc over epoch: 0.8682\n",
            "Cummulative loss: 6741.5991 \n",
            "Validation acc over epoch: 0.8237\n",
            "Cummulative test loss: 1043.302490 \n",
            "Time taken: 368.18s\n",
            "Starting epoch: 2 \n",
            "Training acc over epoch: 0.9438\n",
            "Cummulative loss: 3331.1431 \n",
            "Validation acc over epoch: 0.8494\n",
            "Cummulative test loss: 1094.466553 \n",
            "Time taken: 374.33s\n",
            "Starting epoch: 3 \n",
            "Training acc over epoch: 0.9701\n",
            "Cummulative loss: 1967.4973 \n",
            "Validation acc over epoch: 0.8604\n",
            "Cummulative test loss: 1172.524902 \n",
            "Time taken: 382.57s\n",
            "Starting epoch: 4 \n",
            "Training acc over epoch: 0.9800\n",
            "Cummulative loss: 1387.3806 \n",
            "Validation acc over epoch: 0.8656\n",
            "Cummulative test loss: 1228.378052 \n",
            "Time taken: 376.69s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2iWMNGQahsc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save weights to a Tensorflow Checkpoint file\n",
        "model.save_weights('./skip_gram_weights_wiki_5000000')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb7Wx_rzZOOm",
        "colab_type": "text"
      },
      "source": [
        "### Word Embeddings Projector\n",
        "\n",
        "For visualising word embeddings, tensorflow offers a brilliant platform that can be used to load and visulaise saved weights vector with just a couple lines of code! Here's how we do it\n",
        " \n",
        "\n",
        "*   First extract and store the weights of embedding layer\n",
        "*   Then populate the word embeddings as shown below in two files: vecs.tsv which stores the actual vectors and meta.tsv contains associated metadata for visualising\n",
        "\n",
        "After that hop over to http://projector.tensorflow.org/ and load the files created in previous step. That's it! Tensorflow takes care of the rest.\n",
        "\n",
        "I've added the visualizations of learnt word embeddings in this same repository under visualizations folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGpXtNRS-V_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_embeddings_layer = model.layers[0]\n",
        "weights = word_embeddings_layer.get_weights()[0]\n",
        "print(\"Word Embeddings shape: %s\" % (weights.shape,))\n",
        "\n",
        "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
        "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
        "\n",
        "for num, word in tokenizer.index_word.items():\n",
        "  vec = weights[num] # skip 0, it's padding.\n",
        "  out_m.write(word + \"\\n\")\n",
        "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
        "out_v.close()\n",
        "out_m.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi6ZCBqK2B9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download the embeddings to your local system\n",
        "files.download('vecs.tsv')\n",
        "files.download('meta.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIXsVONZFSbL",
        "colab_type": "text"
      },
      "source": [
        "That's all folks! If you've reached this far, thank you for going through the notebook. If I did my job well, hopefully you now have a clear interpretation of what word embeddings are, their various characteristics and how they're computed under the hood. Please let me know if you have any questions. I'll add more notebooks in near future to offer comparison between different well-known word embeddings - word2vec, GloVe, fastText, etc. It'll equip you with the necessary background knowledge required for selecting a word embedding suitable for your task."
      ]
    }
  ]
}